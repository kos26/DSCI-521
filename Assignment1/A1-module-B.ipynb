{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment group 1: Textual feature extraction and numerical comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module B _(35 points)_ Key word in context\n",
    "\n",
    "Key word in context (KWiC) is a common format for concordance lines, i.e., contextualized instances of principal words used in a book. More generally, KWiC is essentially the concept behind the utility of 'find in page' on document viewers and web browsers. This module builds up a KWiC utility for finding key word-containing sentences, and 'most relevant' paragraphs, quickly.\n",
    "\n",
    "__B1.__ _(3 points)_ Start by writing a function called `load_book` that reads in a book based on a provided `book_id` string and returns a list of `paragraphs` from the book. When book data is loaded, you should remove the space characters at the beginning and end of the text (e.g., using `strip()`). Then, to split books into paragraphs, use the `re.split()` method to split the input in cases where there are two or more new lines. Note, that books are in the provided `data/books` directory.\n",
    "\n",
    "Note: this module is not focused on text pre-processing beyond a split into paragraphs; you do _not_ need to remove markup or non-substantive content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B1:Function(3/3)\n",
    "\n",
    "import re\n",
    "\n",
    "def load_book(book_id):\n",
    "    \n",
    "    with open(\"./data/books/\"+book_id+\".txt\", \"r\") as fh:\n",
    "        paragraphs = fh.read()\n",
    "        paragraphs = paragraphs.strip()\n",
    "        paragraphs = re.split(\"\\n\\n\", paragraphs)\n",
    "    return paragraphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test your function, lets apply it to look at a few paragraphs from book 84."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "723\n",
      "These reflections have dispelled the agitation with which I began my\n",
      "letter, and I feel my heart glow with an enthusiasm which elevates me\n",
      "to heaven, for nothing contributes so much to tranquillize the mind as\n",
      "a steady purpose--a point on which the soul may fix its intellectual\n",
      "eye.  This expedition has been the favourite dream of my early years. I\n",
      "have read with ardour the accounts of the various voyages which have\n",
      "been made in the prospect of arriving at the North Pacific Ocean\n",
      "through the seas which surround the pole.  You may remember that a\n",
      "history of all the voyages made for purposes of discovery composed the\n",
      "whole of our good Uncle Thomas' library.  My education was neglected,\n",
      "yet I was passionately fond of reading.  These volumes were my study\n",
      "day and night, and my familiarity with them increased that regret which\n",
      "I had felt, as a child, on learning that my father's dying injunction\n",
      "had forbidden my uncle to allow me to embark in a seafaring life.\n"
     ]
    }
   ],
   "source": [
    "# B1:SanityCheck\n",
    "paragraphs = load_book('84')\n",
    "print(len(paragraphs))\n",
    "print(paragraphs[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__B2.__ _(10 points)_ Next, write a function called `kwic(paragraphs, search_terms)` that accepts a list of string `paragraphs` and a set of `search_term` strings. The function should:\n",
    "\n",
    "1. initialize `data` as a `defaultdict` of lists\n",
    "2. loop over the `paragraphs` and apply `spacy`'s processing to produce a `doc` for each;\n",
    "3. loop over the `doc.sents` resulting from each `paragraph`;\n",
    "4. loop over the words in each `sentence`;\n",
    "5. check: `if` a `word` is `in` the `search_terms` set;\n",
    "6. `if` (5), then `.append()` the reference under `data[word]` as a list: `[[i, j, k], sentence]`, where `i`, `j`, and `k` refer to the paragraph-in-book, sentence-in-paragraph, and word-in-sentence indices, respectively.\n",
    "\n",
    "Your output, `data`, should then be a default dictionary of lists of the format:\n",
    "```\n",
    "data['word'] = [[[i, j, k], [\"These\", \"are\", \"sentences\", \"containing\", \"the\", \"word\", \"'word'\", \".\"]],\n",
    "                ...,]\n",
    "```\n",
    "\n",
    "Note, we have imported spacy and set it up to use the `\"en\"` model. This will require you to install spacy by running `pip install spacy` and downloading the `\"en\"` model by running the command `python -m spacy download en`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B2:Function(10/10)\n",
    "\n",
    "from collections import defaultdict\n",
    "import spacy\n",
    "nlp = spacy.load(\"en\")\n",
    "\n",
    "def kwic(paragraphs, search_terms = {}):\n",
    "    \n",
    "    data = defaultdict(list)\n",
    "    for i in range(len(paragraphs)):\n",
    "        doc = nlp(paragraphs[i])\n",
    "        paragraph_list = list(doc.sents)\n",
    "        for j in range(len(paragraph_list)):\n",
    "            sentence_list = str(paragraph_list[j]).split()\n",
    "            for k in range(len(sentence_list)):\n",
    "                if sentence_list[k] in search_terms:\n",
    "                    data[sentence_list[k]].append([[i,j,k], paragraph_list[j].text.split()])\n",
    "    return data\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's test your function using the paragraphs from your `load_book` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {'Ocean': [[[10, 2, 24],\n",
       "               ['I',\n",
       "                'have',\n",
       "                'read',\n",
       "                'with',\n",
       "                'ardour',\n",
       "                'the',\n",
       "                'accounts',\n",
       "                'of',\n",
       "                'the',\n",
       "                'various',\n",
       "                'voyages',\n",
       "                'which',\n",
       "                'have',\n",
       "                'been',\n",
       "                'made',\n",
       "                'in',\n",
       "                'the',\n",
       "                'prospect',\n",
       "                'of',\n",
       "                'arriving',\n",
       "                'at',\n",
       "                'the',\n",
       "                'North',\n",
       "                'Pacific',\n",
       "                'Ocean',\n",
       "                'through',\n",
       "                'the',\n",
       "                'seas',\n",
       "                'which',\n",
       "                'surround',\n",
       "                'the',\n",
       "                'pole.']]],\n",
       "             'seas': [[[10, 2, 27],\n",
       "               ['I',\n",
       "                'have',\n",
       "                'read',\n",
       "                'with',\n",
       "                'ardour',\n",
       "                'the',\n",
       "                'accounts',\n",
       "                'of',\n",
       "                'the',\n",
       "                'various',\n",
       "                'voyages',\n",
       "                'which',\n",
       "                'have',\n",
       "                'been',\n",
       "                'made',\n",
       "                'in',\n",
       "                'the',\n",
       "                'prospect',\n",
       "                'of',\n",
       "                'arriving',\n",
       "                'at',\n",
       "                'the',\n",
       "                'North',\n",
       "                'Pacific',\n",
       "                'Ocean',\n",
       "                'through',\n",
       "                'the',\n",
       "                'seas',\n",
       "                'which',\n",
       "                'surround',\n",
       "                'the',\n",
       "                'pole.']],\n",
       "              [[668, 9, 17],\n",
       "               ['I',\n",
       "                'had',\n",
       "                'determined,',\n",
       "                'if',\n",
       "                'you',\n",
       "                'were',\n",
       "                'going',\n",
       "                'southwards,',\n",
       "                'still',\n",
       "                'to',\n",
       "                'trust',\n",
       "                'myself',\n",
       "                'to',\n",
       "                'the',\n",
       "                'mercy',\n",
       "                'of',\n",
       "                'the',\n",
       "                'seas',\n",
       "                'rather',\n",
       "                'than',\n",
       "                'abandon',\n",
       "                'my',\n",
       "                'purpose.']],\n",
       "              [[676, 17, 4],\n",
       "               ['Behold,',\n",
       "                'on',\n",
       "                'these',\n",
       "                'desert',\n",
       "                'seas',\n",
       "                'I',\n",
       "                'have',\n",
       "                'found',\n",
       "                'such',\n",
       "                'a',\n",
       "                'one,',\n",
       "                'but',\n",
       "                'I',\n",
       "                'fear',\n",
       "                'I',\n",
       "                'have',\n",
       "                'gained',\n",
       "                'him',\n",
       "                'only',\n",
       "                'to',\n",
       "                'know',\n",
       "                'his',\n",
       "                'value',\n",
       "                'and',\n",
       "                'lose',\n",
       "                'him.']]]})"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# B2:SanityCheck\n",
    "kwic(paragraphs, {'Ocean', 'seas'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__B3.__ _(2 points)_ Let's test your `kwic` search function's utility using the pre-processed `paragraphs` from book `84` for the key words `Frankenstein` and `monster` in context. Answer the inline questions about these tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of sentences 'Frankenstein' appears in: 9\n",
      "# of sentences 'monster' appears in: 21\n",
      "\n",
      "I was at first touched by the expressions of his misery; yet, when I called to mind what Frankenstein had said of his powers of eloquence and persuasion, and when I again cast my eyes on the lifeless form of my friend, indignation was rekindled within me.\n",
      "\n",
      "I started from my sleep with horror; a cold dew covered my forehead, my teeth chattered, and every limb became convulsed; when, by the dim and yellow light of the moon, as it forced its way through the window shutters, I beheld the wretch--the miserable monster whom I had created.\n"
     ]
    }
   ],
   "source": [
    "# B3:SanityCheck\n",
    "results = kwic(paragraphs, {\"Frankenstein\", \"monster\"})\n",
    "\n",
    "print(\"# of sentences 'Frankenstein' appears in: {}\".format(len(results['Frankenstein'])))\n",
    "print(\"# of sentences 'monster' appears in: {}\".format(len(results['monster'])))\n",
    "print()\n",
    "\n",
    "print(\" \".join(results['Frankenstein'][7][1]))\n",
    "print()\n",
    "print(\" \".join(results['monster'][0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slow\n"
     ]
    }
   ],
   "source": [
    "# B3:Inline(1/2)\n",
    "\n",
    "# Is the kwic function fast or slow? Print \"Fast\" or \"Slow\"\n",
    "print(\"Slow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "# B3:Inline(1/2)\n",
    "\n",
    "# How many sentences does the work Frankenstein appear in? Print the integer (0 is just a placeholder).\n",
    "print(9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__B4.__ _(10 pts)_ The cost of _indexing_ a given book turns out to be the limiting factor here for kwic. Presently, we have our pre-processing `load_book` function just splitting a document into paragraphs. Rewrite the `load_book` function to do some additional preprocessing. Specifically, this function should be modified to:\n",
    "\n",
    "1. split a `book` into paragraphs and loop over them, but\n",
    "2. process each paragraph with `spacy`;\n",
    "3. store the `document` as a triple-nested list, so that each word _string_ is reachable via three indices: `word = document[i][j][k]`;\n",
    "4. record an `index = defaultdict(list)` containing a list of `[i,j,k]` lists for each word; and\n",
    "5. `return document, index`\n",
    "\n",
    "Pre-computing the `index` will allow us to efficiently look up the locations of each word's instance in `document`, and the triple-list format of our document will allow us fast access to extract the sentence for KWiC. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B4:Function(10/10)\n",
    "\n",
    "def load_book(book_id):\n",
    "    \n",
    "    with open(\"./data/books/\"+book_id+\".txt\", \"r\") as fh:\n",
    "        document = []\n",
    "        index = defaultdict(list)\n",
    "        paragraphs = fh.read()\n",
    "        paragraphs = paragraphs.strip()\n",
    "        paragraphs = re.split(\"\\n\\n\", paragraphs)\n",
    "        for i in range(len(paragraphs)):\n",
    "            paragraph = nlp(paragraphs[i])\n",
    "            paragraph_list = []\n",
    "            p = list(paragraph.sents)\n",
    "            for j in range(len(p)):\n",
    "                sentences = str(p[j]).split()\n",
    "                sentences_list = []\n",
    "                for k in range(len(sentences)):\n",
    "                    sentences_list.append(sentences[k])\n",
    "                    index[sentences[k]].append([i,j,k])\n",
    "                paragraph_list.append(sentences_list) \n",
    "            document.append(paragraph_list)\n",
    "    return(document, index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's test your new function on `book_id` = `'84'`. We'll use the returned document to access a particular sentence and print out the `[i,j,k]` locations of the word `'monster'` from `index`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B4:SanityCheck\n",
    "\n",
    "# load the book\n",
    "document, index = load_book(\"84\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['There,', 'Margaret,', 'the', 'sun', 'is', 'forever', 'visible']"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# B4:SanityCHeck\n",
    "\n",
    "# Output paragraph 9, sentence 5\n",
    "document[9][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[124, 10, 45],\n",
       " [139, 3, 4],\n",
       " [142, 1, 4],\n",
       " [243, 3, 24],\n",
       " [261, 3, 16],\n",
       " [321, 1, 31],\n",
       " [380, 13, 5],\n",
       " [477, 7, 8],\n",
       " [478, 7, 4],\n",
       " [510, 1, 7],\n",
       " [527, 0, 1],\n",
       " [538, 19, 15],\n",
       " [560, 3, 28],\n",
       " [587, 11, 63],\n",
       " [606, 3, 2],\n",
       " [615, 2, 8],\n",
       " [639, 1, 16],\n",
       " [653, 8, 5],\n",
       " [673, 0, 34],\n",
       " [673, 2, 2],\n",
       " [709, 11, 1]]"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# B4:SanityCheck\n",
    "\n",
    "# Output the indices for monster\n",
    "index['monster']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__B5.__ _(5 pts)_ Finally, make a new function called `fast_kwic` that takes a `document` and `index` from our new `load_book` function as well as a provided list of `search_terms` (just like our original kwic function). The function should loops through all specified `search_terms` to identify indices from `index[word]` for the key word-containing sentences and use them to extract these sentences from `document` into the same data structure as output by __B2__:\n",
    "```\n",
    "data['word'] = [[[i, j, k], [\"These\", \"are\", \"sentences\", \"containing\", \"the\", \"word\", \"'word'\", \".\"]],\n",
    "                ...,]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B5:Function(5/5)\n",
    "\n",
    "def fast_kwic(document, index, search_terms = {}):\n",
    "    \n",
    "    data = defaultdict(list)\n",
    "    for i in search_terms:\n",
    "        if index[i]:\n",
    "            indices = index[i]\n",
    "            for j in indices:\n",
    "                data[i].append([[j[0], j[1], j[2]], document[j[0]][j[1]]])\n",
    "            \n",
    "    \n",
    "    return(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test our new function, lets test it on the same keywords as before: `Frankenstein` and `monster`. Note that the output from this sanity check should be the same as the one from **B3**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of sentences 'Frankenstein' appears in: 9\n",
      "# of sentences 'monster' appears in: 21\n",
      "\n",
      "I was at first touched by the expressions of his misery; yet, when I called to mind what Frankenstein had said of his powers of eloquence and persuasion, and when I again cast my eyes on the lifeless form of my friend, indignation was rekindled within me.\n",
      "\n",
      "I started from my sleep with horror; a cold dew covered my forehead, my teeth chattered, and every limb became convulsed; when, by the dim and yellow light of the moon, as it forced its way through the window shutters, I beheld the wretch--the miserable monster whom I had created.\n"
     ]
    }
   ],
   "source": [
    "# B5:SanityCheck\n",
    "\n",
    "fast_results = fast_kwic(document, index, {'Frankenstein', 'monster'})\n",
    "\n",
    "print(\"# of sentences 'Frankenstein' appears in: {}\".format(len(fast_results['Frankenstein'])))\n",
    "print(\"# of sentences 'monster' appears in: {}\".format(len(fast_results['monster'])))\n",
    "print()\n",
    "\n",
    "print(\" \".join(fast_results['Frankenstein'][7][1]))\n",
    "print()\n",
    "print(\" \".join(fast_results['monster'][0][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__B6.__ _(5 pts)_ Your goal here is to modify the pre-processing in `load_book` one more time! Make a small modification to the input: `load_book(book_id, pos = True, lemma = True):`, to accept two boolean arguments, `pos` and `lemma` specifying how to identify each word as a key term. In particular, each word will now be represented in both of the `document` and `index` as a tuple: `heading = (text, tag)`, where `text` contains the `word.text` attribute from `spacy` if `lemma = False`, and `word.lemma_` attribute if `True`. Similarly, `tag` should be left empty as `\"\"` if `pos = False` and otherwise contain `word.pos_`.\n",
    "\n",
    "Note this functions output should still consist of a `document` and `index` in the same format aside from the replacement of `word` with `heading`, which will allow for the same use of output in `fast_kwic`, although more specified by the textual features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B6:Function(5/5)\n",
    "\n",
    "def load_book(book_id, pos = True, lemma = True):\n",
    "    \n",
    "    with open(\"./data/books/\"+book_id+\".txt\", \"r\") as fh:\n",
    "        document = []\n",
    "        index = defaultdict(list)\n",
    "        paragraphs = fh.read()\n",
    "        paragraphs = paragraphs.strip()\n",
    "        paragraphs = re.split(\"\\n\\n\", paragraphs)\n",
    "        i = 0\n",
    "        for paragraph in paragraphs:\n",
    "            paragraph = nlp(paragraph)\n",
    "            paragraph_list = []\n",
    "            j = 0\n",
    "            for sentences in list(paragraph.sents):\n",
    "                sentences_list = []\n",
    "                k = 0\n",
    "                for words in sentences:\n",
    "                    sentences_list.append((words.text).strip())\n",
    "                    if (pos == True) and (lemma == True):\n",
    "                        index[words.lemma_, words.pos_].append([i,j,k])\n",
    "                    elif (pos == False) and (lemma == True):\n",
    "                        index[words.lemma_, \" \"].append([i,j,k])\n",
    "                    elif (pos == True) and (lemma == False):\n",
    "                        index[words.text, words.pos_].append([i,j,k])\n",
    "                    else:\n",
    "                        index[words.text, \" \"].append([i,j,k])\n",
    "                    k+=1\n",
    "                paragraph_list.append(sentences_list)\n",
    "                j+=1\n",
    "            document.append(paragraph_list)\n",
    "            i+=1\n",
    "    \n",
    "    return document, index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B6:SanityCheck\n",
    "document, index = load_book(\"84\", pos = True, lemma = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence with ('cold', 'NOUN'):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The  cold is not excessive , if you are wrapped in furs -- a dress which I have  already adopted , for there is a great difference between walking the  deck and remaining seated motionless for hours , when no exercise  prevents the blood from actually freezing in your veins . '"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# B6:SanityCheck\n",
    "print(\"Sentence with ('cold', 'NOUN'):\")\n",
    "\" \".join(fast_kwic(document, index, search_terms = {('cold', 'NOUN')})[('cold', 'NOUN')][0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence with ('cold', 'ADJ'):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I am already far north of London , and as I walk in the streets of  Petersburgh , I feel a cold northern breeze play upon my cheeks , which  braces my nerves and fills me with delight . '"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# B6:SanityCheck\n",
    "print(\"Sentence with ('cold', 'ADJ'):\")\n",
    "\" \".join(fast_kwic(document, index, search_terms = {('cold', 'ADJ')})[('cold', 'ADJ')][0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
